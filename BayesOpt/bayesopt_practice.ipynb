{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bayesian optimization (BO)\n",
    "\n",
    "Content\n",
    "1. Bayesian optimization overview\n",
    "2. One dimensional example\n",
    "3. AutoML: optimization of hyperparameters for machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Bayesian optimization?\n",
    "\n",
    "* Optimization of \"heavy\" functions \n",
    "* The target function is a blackbox, typically noisy, while smooth\n",
    "\n",
    "\n",
    "* Construction a regression model using available data\n",
    "* Take into account uncertainty of the regression model\n",
    "* Gaussian process regression is OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frameworks\n",
    "\n",
    "We need libraries for\n",
    "* Gaussian process regression **[GPyTorch](https://gpytorch.ai/)** (see previous seminar)\n",
    "* Gaussian process regression-based Bayesian optimization **[BoTorch](https://botorch.org/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment and run the following celll to install BoTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# auxiliary functions\n",
    "import utils\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One dimensional example\n",
    "\n",
    "We demonstrate concepts using one-dimensional example.\n",
    "\n",
    "Let us consider Bayesian optimization for one-dimensional function **Forrester**:\n",
    "$$\n",
    "f(x) = (6 x - 2)^2 \\sin(12 x - 4).\n",
    "$$\n",
    "\n",
    "The optimization problem is the following:\n",
    "$$\n",
    "f(x) \\rightarrow \\min, x \\in [0, 1].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = 'cpu'\n",
    "dtype = torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forrester(x):\n",
    "    \"\"\"\n",
    "    Forrester function\n",
    "        \n",
    "        y = (6x - 2)^2 \\sin(12x - 4), x \\in [0, 1]\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.tensor, shape=(n_samples,)\n",
    "        Input values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : torch.tensor\n",
    "        Output values\n",
    "    \"\"\"\n",
    "    y = (6 * x - 2)**2 * torch.sin(12 * x - 4)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 300)\n",
    "plt.plot(x, forrester(torch.tensor(x)).numpy())\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Forrester function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization workflow:\n",
    "1. Construct a regression model $\\hat{f}(x)$ of a function $f(x)$ using the sample $D = \\{(x_i, f(x_i))\\}_{i = 1}^n$\n",
    "2. Select a new point that maximize an acquisition function\n",
    "$$\n",
    "x_{new} = \\arg\\max\\limits_x a(x)\n",
    "$$\n",
    "3. Calculate $f(x_{new})$ at the new point.\n",
    "4. Add the pair $(x_{new}, f(x_{new}))$ to the sample $D$.\n",
    "5. Update the model $\\hat{f}(x)$ and go to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have a black-box objective function. Now we define components needed for BO\n",
    "\n",
    "1. What kind of the regression model we need\n",
    "2. What kind of the acquisition function we use\n",
    "3. How do we optimize the acquisition function\n",
    "4. Should we use sequential or batch optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Let's create initial design and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_data(n=10):\n",
    "    X = torch.rand(n, 1, device=device, dtype=dtype)\n",
    "    y = forrester(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_initial_data(n=5)\n",
    "utils.plot_1D_function(forrester, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create GP model and fit it using initial design\n",
    "\n",
    "For BO we need a model that return a probability distribution for each point X.  \n",
    "The most widely used model is Gaussian Processes based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "\n",
    "\n",
    "def initialize_model(X, y, GP=None, state_dict=None, *GP_args, **GP_kwargs):\n",
    "    \"\"\"\n",
    "    Create GP model and fit it. The function also accepts\n",
    "    state_dict which is used as an initialization for the GP model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.tensor, shape=(n_samples, dim)\n",
    "        Input values\n",
    "        \n",
    "    y : torch.tensor, shape=(n_samples,)\n",
    "        Output values\n",
    "        \n",
    "    GP : botorch.models.Model\n",
    "        GP model class\n",
    "        \n",
    "    state_dict : dict\n",
    "        GP model state dict\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mll : gpytorch.mlls.MarginalLoglikelihood\n",
    "        Marginal loglikelihood\n",
    "    \n",
    "    gp : \n",
    "    \"\"\"\n",
    "\n",
    "    if GP is None:\n",
    "        GP = SingleTaskGP\n",
    "        \n",
    "    model = GP(X, y, *GP_args, **GP_kwargs).to(X)\n",
    "\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    # load state dict if it is passed\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll, gp = initialize_model(X, y)\n",
    "fit_gpytorch_model(mll);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create acquisition function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of the acquisation functions \n",
    "\n",
    "#### Upper confidence bound (UCB) \n",
    "\n",
    "$$\n",
    "UÐ¡B(x) = \\hat{f}(x) + \\beta \\hat{\\sigma}(x),\n",
    "$$\n",
    "$\\hat{f}(x), \\hat{\\sigma}(x)$ - mean and standard deviation of the Gaussian process regression model at $x$.\n",
    "\n",
    "#### Probability of Improvement (PI)\n",
    "\n",
    "$$\n",
    "PI(x) = P \\left ( \\hat{f}(x) < f_{min} \\right )\n",
    "$$\n",
    "\n",
    "#### Expected Improvement (EI) \n",
    "\n",
    "$$\n",
    "EI(x) = \\mathbb{E}_{p(\\hat{f})} (f_{min} - \\hat{f}(x))_+.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition import ExpectedImprovement\n",
    "\n",
    "acquisition = ExpectedImprovement(gp, y.min(), maximize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimize acquisition function to obtain new candidate points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim import joint_optimize, sequential_optimize\n",
    "\n",
    "bounds = torch.tensor([[0], [1]]).to(X)\n",
    "candidate = joint_optimize(\n",
    "    acquisition, bounds=bounds, q=10, num_restarts=5, raw_samples=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_acquisition(acquisition, X, y, candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Update the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data set\n",
    "X = torch.cat([X, candidate])\n",
    "y = torch.cat([y, forrester(candidate)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update the GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update GP model\n",
    "mll, gp = initialize_model(X, y, state_dict=gp.state_dict())\n",
    "fit_gpytorch_model(mll);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Repeat from step 1\n",
    "\n",
    "**Task** implement `bo_step()` function below. Steps:\n",
    "1. Create and fit GP model using given `X, y, GP` (`GP` is a model constructor that takes as input pair `X` and `y`)\n",
    "2. Create acquisition function using constructed model and `acquisition` (this is an acquisiton function constructor that takes as input gp model)\n",
    "3. Optimize acquisition function to find new `candidate`\n",
    "4. Update `X, y` by appending to `candidate`\n",
    "5. Return updated `X, y` and the GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo_step(X, y, objective, bounds, GP=None, acquisition=None, q=1, state_dict=None, plot=False):\n",
    "    \"\"\"\n",
    "    One iteration of Bayesian optimization:\n",
    "        1. Fit GP model using (X, y)\n",
    "        2. Create acquisition function\n",
    "        3. Optimize acquisition function to obtain candidate point\n",
    "        4. Evaluate objective at candidate point\n",
    "        5. Add new point to the data set\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : torch.tensor, shape=(n_samples, dim)\n",
    "        Input values\n",
    "        \n",
    "    y : torch.tensor, shape=(n_samples,)\n",
    "        Objective values\n",
    "        \n",
    "    objective : callable, argument=torch.tensor\n",
    "        Objective black-box function, accepting as an argument torch.tensor\n",
    "        \n",
    "    bounds : torch.tensor, shape=(2, dim)\n",
    "        Box-constraints\n",
    "    \n",
    "    GP : callable\n",
    "        GP model class constructor. It is a function that takes as input\n",
    "        2 tensors - X, y - and returns an instance of botorch.models.Model.\n",
    "       \n",
    "    acquisition : callable\n",
    "        Acquisition function construction. It is a function that receives\n",
    "        one argument - GP model - and returns an instance of\n",
    "        botorch.acquisition.AcquisitionFunction\n",
    "        \n",
    "    q : int\n",
    "        Number of candidate points to find\n",
    "        \n",
    "    state_dict : dict\n",
    "        GP model state dict\n",
    "        \n",
    "    plot : bool\n",
    "        Flag indicating whether to plot the result\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : torch.tensor\n",
    "        Tensor of input values with new point\n",
    "        \n",
    "    y : torch.tensor\n",
    "        Tensor of output values with new point\n",
    "        \n",
    "    gp : botorch.models.Model\n",
    "        Constructed GP model\n",
    "    \n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> from botorch.models import FixedNoiseGP\n",
    "    >>> noise_var = 1e-2 * torch.ones_like(y)\n",
    "    >>> GP = lambda X, y: FixedNoiseGP(X, y, noise_var)\n",
    "    >>> acq_func = labmda gp: ExpectedImprovement(gp, y.min(), maximize=False)\n",
    "    >>> X, y = bo_step(X, y, objective, GP=GP, Acquisition=acq_func)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### Your code goes here ###\n",
    "    \n",
    "    gp = ... # Save gp model in this variable\n",
    "    X = ...  # Save updated input points in this variable\n",
    "    y = ...  # Save updated objective values in this variable\n",
    "    \n",
    "    ### Your code ends here ###\n",
    "    \n",
    "    if plot:\n",
    "        utils.plot_acquisition(acquisition, X, y, candidate)\n",
    "        \n",
    "    return X, y, gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = forrester\n",
    "bounds = torch.tensor([[0], [1]], dtype=dtype)\n",
    "acquisition = lambda gp: ExpectedImprovement(gp, y.min(), maximize=False)\n",
    "\n",
    "X, y, gp = bo_step(X, y, forrester, bounds, GP=None,\n",
    "                   acquisition=acquisition, plot=True,\n",
    "                   state_dict=gp.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    acquisition = lambda gp: ExpectedImprovement(gp, y.min(),\n",
    "                                                 maximize=False)\n",
    "    X, y, gp = bo_step(X, y, forrester, bounds, GP=SingleTaskGP,\n",
    "                       acquisition=acquisition, plot=True,\n",
    "                       state_dict=gp.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results\n",
    "\n",
    "Two criteria\n",
    "1. Distance between consecutive x's\n",
    "2. Current best value of the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_convergence(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Obtained xmin:  {X[-1].item():.3f}, real xmin: 0.780 (approximate)')\n",
    "print(f'Obtained fmin: {y[-1].item():.3f}, real fmin: -6.000 (approximate)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Confidence Bound (LCB) acquisition function\n",
    "Upper Confidence Bound (UCB) is for maximization. LCB\n",
    "$$\n",
    "    LCB(x) = \\hat{f}(x) - \\beta \\hat{\\sigma}(x)\n",
    "$$\n",
    "is for minimization.\n",
    "\n",
    "**Question**: Should we maximize or minimize the LCB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.acquisition import UpperConfidenceBound\n",
    "\n",
    "class LowerConfidenceBound(UpperConfidenceBound):\n",
    "    def __init__(self, gp, beta):\n",
    "        # When maximize==True UCB returns (\\mu - beta * \\sigma)\n",
    "        # though it is still maximized\n",
    "        super().__init__(gp, beta, maximize=False)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # The criterion is maximized, so we flip the sign\n",
    "        # to find argmin\n",
    "        lcb = super().forward(X)\n",
    "        return -lcb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Optimize forrester function using LCB acquisition function.  \n",
    "What happens if $\\beta$ is small? Large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_initial_data(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bayesian optimization for the parameters of Gradient boosting\n",
    "\n",
    "Now we optimize hyperparameters for Gradient boosting of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If lightgbm is not installed, please run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We predict defaults for the classification problem\n",
    "\n",
    "The goal is to predict if the two years absense of payments occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training sample\n",
    "data = pd.read_csv('training_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('SeriousDlqin2yrs', axis=1)\n",
    "y = data['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define objective function\n",
    "\n",
    "The objective function should take `torch.tensor`, convert it to a `list` of `dictionaries` with model parameters\n",
    "and for each set of model parameters calculate cross-validation score of the model\n",
    "\n",
    "Note, that we constrain the search space to [0, 1]-hypercube, but inside the objective function rescale the parameters\n",
    "to the correct bounds.\n",
    "We do this to obtain better GP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from botorch.utils.transforms import normalize, unnormalize\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def wrap_X(X, space):\n",
    "    \"\"\"\n",
    "    Wrap tensor to a list of dictionaries\n",
    "    \n",
    "    Parameters:\n",
    "    X : torch.tensor, shape=(n_samples, dim)\n",
    "        Tensor of parameters values\n",
    "        \n",
    "    space : dict\n",
    "        Search space description\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    wrapped_X : lsit[dict{param_name: value}]\n",
    "        List of dictionary of parameters\n",
    "    \"\"\"\n",
    "    def _wrap_row(row):\n",
    "        wrapped_row = {}\n",
    "        for i, x in enumerate(row):\n",
    "            wrapped_row[space[i]['name']] = x.item()\n",
    "        \n",
    "            if space[i]['type'] == 'discrete':\n",
    "                wrapped_row[space[i]['name']] = int(np.round(x.item()))\n",
    "        return wrapped_row\n",
    "    \n",
    "    wrapped_X = []\n",
    "    for i in range(X.shape[0]):\n",
    "        wrapped_X.append(_wrap_row(X[i]))\n",
    "        \n",
    "    return wrapped_X\n",
    "\n",
    "\n",
    "def unwrap_X(parameters, space):\n",
    "    \"\"\"\n",
    "    Unwrap list of dictionaries to torch.tensor\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : list(dict)\n",
    "        List of parameters\n",
    "        \n",
    "    space : dict\n",
    "        Input space definition\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    unwrapped_X : torch.tensor\n",
    "        Tensor of parameter values\n",
    "    \"\"\"\n",
    "    X = torch.zeros(len(parameters), len(space),\n",
    "                    dtype=torch.float64)\n",
    "    for i, p in enumerate(parameters):\n",
    "        x = [p[var['name']] for var in space]\n",
    "        X[i] = torch.tensor(x, dtype=torch.float64)\n",
    "        \n",
    "    return X\n",
    "\n",
    "\n",
    "def get_cv_quality(parameters, space):\n",
    "    \"\"\"\n",
    "  \n",
    "    Quality of model using given hyperparameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : torch.tensor, shape=(n_samples, dim)\n",
    "        Tensor of hyperparameteres\n",
    "        \n",
    "    space : dict\n",
    "        Input space description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    roc_auc : torch.tensor\n",
    "        Tensor of ROC AUC values\n",
    "    \"\"\"\n",
    "\n",
    "    # Rescale to original bounds\n",
    "    dtype = parameters.dtype\n",
    "    device = parameters.device\n",
    "    bounds = torch.tensor([var['domain'] for var in space]).to(parameters).t()\n",
    "    parameters = unnormalize(parameters, bounds)\n",
    "\n",
    "    # Convert tensor to a list of dictionaries\n",
    "    parameters = wrap_X(parameters, space)\n",
    "    \n",
    "    def _get_score(params_dict):\n",
    "        model = lgb.LGBMClassifier(**params_dict, n_jobs=-1)\n",
    "        score = np.mean(cross_validate(model, X, y, cv=5, n_jobs=-1,\n",
    "                                       scoring='roc_auc')['test_score'])\n",
    "        return score\n",
    "    \n",
    "    score_list = Parallel(-1)(delayed(_get_score)(p) for p in parameters)\n",
    "\n",
    "    return torch.tensor(score_list, dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the search space\n",
    "\n",
    "For each parameter we defin\n",
    "* type - \"discrete\" or \"continuous\"\n",
    "* domain - box-constraints for the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the region\n",
    "space = [\n",
    "    {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.05, 0.2)},\n",
    "    {'name': 'n_estimators', 'type': 'discrete', 'domain': (50, 300)},\n",
    "    {'name': 'max_depth', 'type': 'discrete', 'domain': (1, 30)},\n",
    "    {'name': 'gamma', 'type': 'continuous', 'domain': (0,  5)},\n",
    "    {'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 10)}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "Generate `5` initial points. Remember, they should be in `[0, 1]` interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Your code goes here ###\n",
    "\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "\n",
    "bounds_01 = torch.zeros(2, len(space), dtype=torch.float64)\n",
    "bounds_01[1] = 1\n",
    "\n",
    "init_X = draw_sobol_samples(bounds_01, 5, 1).squeeze()\n",
    "init_y = get_cv_quality(init_X, space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q-Expected Improvement\n",
    "\n",
    "In previous examples we conducted *sequential* optimization, i.e. we generated candidates sequentially one-by-one.  \n",
    "However, we can generate $q$-candidates at a time.  \n",
    "For this we need special acquisition function, e.g. batched version of Expected Improvement\n",
    "$$\n",
    "    {\\rm qEI}(x) = \\mathbb{E} \\left ( \\max_{i \\in \\{1, \\ldots, q\\}} ( f_{min} - \\hat{f}(x_i)) \\right )_+\n",
    "$$\n",
    "\n",
    "**Pros**\n",
    "* Faster (in wall time) if objective function evaluation can be parallelized\n",
    "* Small $q$ values (~5) gives the same performance as sequential optimization\n",
    "\n",
    "**Cons**\n",
    "* Large $q$ suffers from harder optimization problem\n",
    "* $q$-acquisition functions are intractable, we need to use approximations. In BoTorch they are evaluated using (q-)MC sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bayesian optimization\n",
    "\n",
    "#### Task\n",
    "\n",
    "Run bayesian optimization.\n",
    "* Try to use sequential optimization (as we did in previous tasks)\n",
    "* Try to apply $q$-ExpectedImprovement\n",
    "    * use `botorch.acquisition.qExpectedImprovement`\n",
    "    * pass `q` argument to `bo_step()` function\n",
    "* What about performance of each of 2 approaches (you need to run several times and calculate mean and std)? Wall time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from botorch.acquisition import NoisyExpectedImprovement, qExpectedImprovement\n",
    "from botorch.models import FixedNoiseGP\n",
    "\n",
    "\n",
    "params = init_X\n",
    "scores = init_y\n",
    "state_dict = None\n",
    "\n",
    "q = 5\n",
    "budget = 100\n",
    "\n",
    "objective = lambda x: get_cv_quality(x, space)\n",
    "\n",
    "with tqdm.tqdm(total=budget) as bar:\n",
    "    while len(scores) < budget:\n",
    "        \n",
    "        n_samples = len(scores)\n",
    "    \n",
    "        ### Your code goes here ###\n",
    "\n",
    "        \n",
    "        ### Your code ends here ###\n",
    "        \n",
    "        bar.update(len(scores) - n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_convergence(params, scores, maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_params(params, scores, space):\n",
    "    bounds = torch.tensor([var['domain'] for var in space]).to(params).t()\n",
    "    params = unnormalize(params, bounds)\n",
    "    \n",
    "    best_idx = np.argmax(scores.cpu().numpy())\n",
    "    return wrap_X(params[[best_idx]], space)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate parameters using a test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing sample\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "X_test = test_data.drop('SeriousDlqin2yrs', axis=1)\n",
    "y_test = test_data['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = lgb.LGBMClassifier()\n",
    "initial_model.fit(X, y)\n",
    "proba_predicted = initial_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"Default parameters: ROC AUC = {roc_auc_score(y_test, proba_predicted):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = get_best_params(params, scores, space)\n",
    "best_model = lgb.LGBMClassifier(**best_param)\n",
    "best_model.fit(X, y)\n",
    "proba_predicted = best_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"Optimized parameters: ROC AUC = {roc_auc_score(y_test, proba_predicted):.3f}\")\n",
    "print(f\"Parameters: {best_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Parzen Estimator (TPE)\n",
    "\n",
    "Expected Improvement can be calculated without GP model\n",
    "\n",
    "$$\n",
    "p(y | x) \\propto p(x | y) p(y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x | y) = \\begin{cases}\n",
    "    l(x), \\mbox{ if} y < y^* \\\\\n",
    "    g(x), \\mbox{ if} y \\ge y^* \\\\\n",
    "\\end{cases},\n",
    "$$\n",
    "where $y^*$ is some quantile $\\gamma$ of $y$, i.e., $p(y < y^*) = \\gamma$.\n",
    "\n",
    "$$\n",
    "EI(x) \\propto \\left ( \\gamma + \\frac{g(x)}{l(x)}(1 - \\gamma) \\right )^{-1}\n",
    "$$\n",
    "$l(x)$ and $g(x)$ are estimated using KDE (kernel density estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: prove the expression for EI above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade git+git://github.com/hyperopt/hyperopt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, hp, tpe\n",
    "\n",
    "params_names = ['learning_rate', 'n_estimators', 'subsample']\n",
    "\n",
    "space_hp = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.05, 0.2),\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 100, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 30, 1),\n",
    "    'gamma': hp.uniform('gamma', 0,  5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "}\n",
    "\n",
    "types = {\n",
    "    'learning_rate': 'continuous',\n",
    "    'n_estimators': 'discrete',\n",
    "    'max_depth': 'discrete',\n",
    "    'gamma': 'continuous',\n",
    "    'min_child_weight': 'discrete'\n",
    "}\n",
    "\n",
    "random_state = np.random.RandomState()\n",
    "\n",
    "def wrap_params(params, types):\n",
    "    wrapped_params = params.copy()\n",
    "    for key in wrapped_params:\n",
    "        if types[key] == 'discrete':\n",
    "            wrapped_params[key] = int(wrapped_params[key])\n",
    "\n",
    "    return wrapped_params\n",
    "\n",
    "\n",
    "best = fmin(lambda x: -get_cv_quality(unwrap_X([wrap_params(x, types)], space),\n",
    "                                     space).numpy()[-1],\n",
    "            space=space_hp,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=55, rstate=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = lgb.LGBMClassifier(**wrap_params(best, types))\n",
    "best_model.fit(X, y)\n",
    "proba_predicted = best_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"Optimized parameters: ROC AUC = {roc_auc_score(y_test, proba_predicted):.3f}\")\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex search spaces\n",
    "\n",
    "Hyperopt can handle tree-structured search spaces.  \n",
    "It is useful when some parameter depends on the value of another parameter,\n",
    "e.g. one parameter is `model_type` (SVM, XGBoost, etc). In this case\n",
    "other parameter depend on this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "space_hp = hp.choice(\n",
    "    'model_type', [\n",
    "        {\n",
    "            'type': 'LightGBM',\n",
    "            'learning_rate': hp.uniform('learning_rate', 0.05, 0.2),\n",
    "            'n_estimators': hp.quniform('n_estimators', 10, 100, 1),\n",
    "            'subsample': hp.uniform('subsample', 0.75, 1.),\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'type': 'LogisticRegression',\n",
    "            'C': hp.lognormal('logreg_C', 1e-6, 1),\n",
    "            'penalty': hp.choice(\n",
    "                'logreg_penalty', [\n",
    "                    'l1',\n",
    "                    'l2',\n",
    "                ]),\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_params(params, types):\n",
    "    wrapped_params = params.copy()\n",
    "    for key in wrapped_params:\n",
    "        if types.get(key, '') == 'discrete':\n",
    "            wrapped_params[key] = int(wrapped_params[key])\n",
    "\n",
    "    return wrapped_params\n",
    "\n",
    "\n",
    "def get_cv_quality(parameters):\n",
    "    \"\"\"\n",
    "  \n",
    "    Quality of model using given hyperparameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : np.array, shape=(n_samples, dim)\n",
    "        Tensor of hyperparameteres\n",
    "        \n",
    "    space : dict\n",
    "        Input space description\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    roc_auc : np.array\n",
    "        Tensor of ROC AUC values\n",
    "    \"\"\"\n",
    "    \n",
    "    score_list = []\n",
    "    for params_dict in parameters:\n",
    "        model_type = params_dict.pop('type')\n",
    "        if model_type == 'LightGBM':\n",
    "            model = lgb.LGBMClassifier(**params_dict)\n",
    "            score = cross_validate(model, X, y, cv=3,\n",
    "                                   scoring='roc_auc')\n",
    "        elif model_type == 'LogisticRegression':\n",
    "            \n",
    "            # Process kernel parameters\n",
    "            model = LogisticRegression(\n",
    "                C=params_dict['C'],\n",
    "                penalty=params_dict['penalty'],\n",
    "                solver='lbfgs' if params_dict['penalty'] == 'l2' else 'liblinear',\n",
    "            )\n",
    "            \n",
    "            Xy = pd.concat([X, y], axis=1).dropna(axis=0)\n",
    "            score = cross_validate(\n",
    "                model, Xy.drop('SeriousDlqin2yrs', axis=1),\n",
    "                Xy['SeriousDlqin2yrs'], cv=3, scoring='roc_auc')\n",
    "      \n",
    "        score = np.mean(score['test_score'])\n",
    "        score_list.append(score)\n",
    "        \n",
    "        params_dict['type'] = model_type\n",
    "\n",
    "    return np.array(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=sklearn.exceptions.ConvergenceWarning)\n",
    "\n",
    "best = fmin(lambda x: -get_cv_quality([wrap_params(x, types)])[-1],\n",
    "            space=space_hp,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=55, rstate=random_state)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = lgb.LGBMClassifier(**wrap_params(best, types))\n",
    "best_model.fit(X, y)\n",
    "proba_predicted = best_model.predict_proba(X_test)[:, 1]\n",
    "print(f\"Optimized parameters: ROC AUC = {roc_auc_score(y_test, proba_predicted):.3f}\")\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
