{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Processes (GP) with GPyTorch\n",
    "\n",
    "In this notebook we are going to use [GPyTorch](https://gpytorch.ai/) library for GP modeling.\n",
    "\n",
    "Why **GPyTorch**?\n",
    "\n",
    "* State of the art GP models\n",
    "* Built on top of pytorch having all its advantages (GPU, autograd, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the following line to install GPyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gpytorch\n",
    "from gpytorch import kernels\n",
    "\n",
    "\n",
    "import utils\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Current documentation of GPyTorch library can be found [here](https://gpytorch.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Process Regression\n",
    "\n",
    "A data set $\\left (X, \\mathbf{y} \\right ) = \\left \\{ (x_i, y_i), \\right \\}_{i = 1}^N$ with inputs $x_i \\in \\mathbb{R}^d$ and output $y_i \\in \\mathbb{R}$ is given.  \n",
    "\n",
    "Assumption:\n",
    "$$\n",
    "y = f(x) + \\varepsilon,\n",
    "$$\n",
    "where $f(x)$ is a Gaussian Process and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$ is a Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Posterior distribution\n",
    "$$\n",
    "y_* | X, \\mathbf{y}, x_* \\sim \\mathcal{N}(m(x_*), \\sigma(x_*)),\n",
    "$$\n",
    "with predictive mean and variance given by\n",
    "$$\n",
    "m(x_*) = \\mathbf{k}^T \\mathbf{K}_y^{-1} \\mathbf{y} = \\sum_{i = 1}^N \\alpha_i k(x_*, x_i),\n",
    "$$\n",
    "$$\n",
    "\\sigma^2(x_*) = k(x_*, x_*) - \\mathbf{k}^T\\mathbf{K}_y^{-1}\\mathbf{k},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{k} = \\left ( k(x_*, x_1), \\ldots, k(x_*, x_N) \\right )^T,\n",
    "$$\n",
    "$$\n",
    "\\mathbf{K}_y = \\|k(x_i, x_j)\\|_{i, j = 1}^N + \\sigma_n^2 \\mathbf{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building GPR model\n",
    "\n",
    "We start with model fitting for an univariate function.\n",
    "The notebook covers:\n",
    "* How to define a covariance function for GP model;\n",
    "* How to create a GP model;\n",
    "* How to train it;\n",
    "* What is noise variance and why it's important.\n",
    "\n",
    "\n",
    "Let's fit GPR model for a function $f(x) = âˆ’ \\cos(\\pi x) + \\sin(4\\pi x)$ in $[0, 1]$,\n",
    "with i.i.d. noise $\\varepsilon \\sim \\mathcal{N}(0, 0.01)$. So, we observe $y(x) = f(x) + \\varepsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1150061746)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "X = torch.linspace(0.05, 0.95, N)\n",
    "y = -torch.cos(np.pi * X) + np.sin(4 * np.pi * X) + torch.randn(N) * 0.1\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X.numpy(), y.numpy(), 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1. Define covariance function\n",
    "\n",
    "The most popular kernel - RBF kernel (`kernels.RBFKernel`) - is defined as follows in gpytorch, $k(x, y) = \\exp\\left ( -\\dfrac{\\|x - y\\|^2}{2l^2}\\right )$,\n",
    "where $l$ is a `lengthscale`.  \n",
    "Usually, there is also a coefficient $A$ that scales the kernel.  \n",
    "In gpytorch we can add the scaling coefficient using `kernels.ScaleKernel` as follows\n",
    "```python\n",
    "kernel = kernels.RBFKernel()\n",
    "kernel = kernels.ScaleKernel(kernel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model. In this case the exact inference is available.\n",
    "class GPRegressor(gpytorch.models.ExactGP):\n",
    "    def __init__(self, X, y, kernel, likelihood=None):\n",
    "        if likelihood is None:\n",
    "            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        \n",
    "        super().__init__(X, y, likelihood)\n",
    "        self.mean = gpytorch.means.ConstantMean()\n",
    "        self.kernel = kernel\n",
    "        self.likelihood = likelihood\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean(x)\n",
    "        covar_x = self.kernel(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self(X)\n",
    "            return self.likelihood(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2. Create GPR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPRegressor(X, y, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters of the covariance function\n",
    "\n",
    "Values of parameters of covariance function can be set like:  `kernel.lengthscale = 0.1`.\n",
    "\n",
    "Let's change the value of `lengthscale` parameter and see how it affects the covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernel(kernel, xlim=None, ax=None):\n",
    "    if xlim is None:\n",
    "        xlim = [-3, 5]\n",
    "    x = torch.linspace(xlim[0], xlim[1], 100)\n",
    "    with torch.no_grad():\n",
    "        K = kernel(x, torch.ones((1))).evaluate().reshape(-1, 1)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "    ax.plot(x.numpy(), K.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k = kernels.RBFKernel()\n",
    "theta = np.asarray([0.2, 0.5, 1, 2, 4, 10])\n",
    "figure, axes = plt.subplots(2, 3, figsize=(8, 4))\n",
    "for t, ax in zip(theta, axes.ravel()):\n",
    "    k.lengthscale = t\n",
    "    plot_kernel(k, ax=ax)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend([t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task\n",
    "Try to change parameters to obtain more accurate model.\n",
    "\n",
    "**Question**: based on the input points `X` can we guess the `lengthscale` approximately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######## Your code goes here ########\n",
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "kernel.base_kernel.lengthscale = 0.1\n",
    "\n",
    "model = GPRegressor(X, y, kernel)\n",
    "### Your code ends here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.likelihood.noise = 0.01\n",
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning parameters of a covariance function\n",
    "\n",
    "The parameters are tuned by maximizing the likelihood. To do it, use `optimize()` method of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "model = GPRegressor(X, y, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "def train(model, X, y, n_epochs=100, fix_noise_variance=None, verbose=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # Use Adam optimizer\n",
    "    if fix_noise_variance is not None:\n",
    "        model.likelihood.noise = fix_noise_variance\n",
    "        training_parameters = [p for name, p in model.named_parameters()\n",
    "                               if not name.startswith('likelihood')]\n",
    "    else:\n",
    "        training_parameters = model.parameters()\n",
    "        \n",
    "    optimizer = torch.optim.Adamax(training_parameters, lr=0.1)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    with tqdm.trange(n_epochs, disable=not verbose) as bar:\n",
    "        for i in bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(X)\n",
    "            loss = -mll(out, y)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "                        \n",
    "            # display progress bar\n",
    "            postfix = dict(Loss=f\"{loss.item():.3f}\",\n",
    "                           noise=f\"{model.likelihood.noise.item():.3}\")\n",
    "            \n",
    "            if (hasattr(model.kernel, 'base_kernel') and\n",
    "                hasattr(model.kernel.base_kernel, 'lengthscale')):\n",
    "                lengthscale = model.kernel.base_kernel.lengthscale\n",
    "                if lengthscale is not None:\n",
    "                    lengthscale = lengthscale.squeeze(0).detach().cpu().numpy()\n",
    "            else:\n",
    "                lengthscale = model.kernel.lengthscale\n",
    "\n",
    "            if lengthscale is not None:\n",
    "                if len(lengthscale) > 1:\n",
    "                    lengthscale_repr = [f\"{l:.3f}\" for l in lengthscale]\n",
    "                    postfix['lengthscale'] = f\"{lengthscale_repr}\"\n",
    "                else:\n",
    "                    postfix['lengthscale'] = f\"{lengthscale[0]:.3f}\"\n",
    "                \n",
    "            bar.set_postfix(postfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, X, y)\n",
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Noise variance\n",
    "\n",
    "Noise variance acts like a regularization in GP models. Larger values of noise variance lead to more smooth model.  \n",
    "\n",
    "**Task**: try to change noise variance to some large value, to some small value and plot the results.\n",
    "\n",
    "To set the noise variance use e.g. `model.Gaussian_noise.variance = 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's generate noisier data and try to fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N = 40\n",
    "X = torch.linspace(0.05, 0.95, N)\n",
    "y = -torch.cos(np.pi * X) + torch.sin(4 * np.pi * X) + torch.randn(N) * 0.5\n",
    "\n",
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "model = GPRegressor(X, y, kernel)\n",
    "\n",
    "train(model, X, y)\n",
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's fix noise variance to some small value and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "\n",
    "model = GPRegressor(X, y, kernel)\n",
    "\n",
    "train(model, X, y, fix_noise_variance=0.1, n_epochs=200)\n",
    "\n",
    "utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate multi-dimensional function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    Rosenbrock is a multidimensional function with arbitrary number of input dimensions\n",
    "    f(x) = \\sum_{i=1}^{d - 1} (100 (x_{i + 1} - x_i^2 )^2 + (1 - x_i)^2).\n",
    "    \"\"\"\n",
    "    x = 0.5 * (4 * x - 2)\n",
    "    y = np.sum((1 - x[:, :-1])**2 +\n",
    "               100 * (x[:, 1:] - x[:, :-1]**2)**2, axis=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def plot_2d_func(func, n_rows=1, n_cols=1, title=None):\n",
    "    grid_size = 100\n",
    "    x_grid = np.meshgrid(np.linspace(0, 1, grid_size), np.linspace(0, 1, grid_size))\n",
    "    x_grid = np.hstack((x_grid[0].reshape(-1, 1), x_grid[1].reshape(-1, 1)))\n",
    "    y = func(x_grid)\n",
    "    fig = plt.figure(figsize=(n_cols * 6, n_rows * 6))\n",
    "    ax = fig.add_subplot(n_rows, n_cols, 1, projection='3d')\n",
    "    ax.plot_surface(x_grid[:, 0].reshape(grid_size, grid_size), x_grid[:, 1].reshape(grid_size, grid_size),\n",
    "                    y.reshape(grid_size, grid_size),\n",
    "                    cmap=cm.jet, rstride=1, cstride=1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here how the function looks like in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_2d_func(rosenbrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set\n",
    "Note that it is 3-dimensional now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 3\n",
    "X = torch.tensor(np.random.rand(300, dim), dtype=torch.float64)\n",
    "y = torch.from_numpy(rosenbrock(X.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Try to approximate Rosenbrock function using RBF kernel. MSE (mean squared error) should be $<10^{-2}$.\n",
    "**Hint**: if results are not good maybe it is due to a bad local minimum. You can do one of the following things:\n",
    "0. Use `double()` for all evaluations\n",
    "1. Try to use different initial values of hyperparameters.\n",
    "2. Constrain model parameters to some reasonable bounds. You can do it for example as follows: \n",
    "\n",
    "```\n",
    "constraint = gpytorch.constraints.Interval(0.0, 1.0)\n",
    "kernel = kernels.RBFKernel(lengthscale_constraint=constraint)\n",
    "```\n",
    "3. What about scaling of the data? Does it affect kernel matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Outputscale: {model.kernel.outputscale.item():.3f}\\n\"\n",
    "      f\"Lengthscale: {model.kernel.base_kernel.lengthscale.detach().cpu().numpy()[0]}\\n\"\n",
    "      f\"Noise variance: {model.likelihood.noise.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: calculate MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.random.rand(3000, dim)\n",
    "y_test = rosenbrock(x_test)\n",
    "model.eval()\n",
    "\n",
    "# Turn off some optimization in gpytorch for more accurate computation of the prediction\n",
    "with gpytorch.settings.fast_computations(False, False, False):\n",
    "    y_pr = model.predict(torch.from_numpy(x_test)).mean.numpy()\n",
    "\n",
    "    \n",
    "### Your code goes here ### \n",
    "# Calculate Mean Squared Error using y_pr as a prediction\n",
    "\n",
    "mse = ...\n",
    "\n",
    "### Your code ends here ###\n",
    "\n",
    "\n",
    "print('\\nMSE: {}'.format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Covariance functions\n",
    "\n",
    "The most popular covariance function is RBF. However, not all the functions can be modelled using RBF covariance function. For example, approximations of discontinuous functions suffer from oscillations, approximation of curvy function may suffer from oversmoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def heaviside(x):\n",
    "    return np.asfarray(x > 0)\n",
    "\n",
    "\n",
    "def rastrigin(x):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray - 2D array in [0, 1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : 1D array of values of Rastrigin function\n",
    "    \"\"\"\n",
    "    scale = 8  # 10.24\n",
    "    x = scale * x - scale / 2\n",
    "    y = 10 * x.shape[1] + (x**2).sum(axis=1) - 10 * np.cos(2 * np.pi * x).sum(axis=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plot_2d_func(rastrigin, 1, 2, title='Rastrigin function')\n",
    "\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = heaviside(x)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x, y)\n",
    "ax.set_title('Heaviside function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of oscillations\n",
    "As you can see there are oscillations in viscinity of discontinuity because we are trying to approximate\n",
    "discontinuous function using infinitily smooth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(1150061746)\n",
    "X = torch.rand(50, 1) * 2 - 1\n",
    "y = torch.tensor(heaviside(X.numpy())).float().squeeze()\n",
    "\n",
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "model = GPRegressor(X, y, kernel)\n",
    "train(model, X, y)\n",
    "\n",
    "utils.plot_model(model)\n",
    "plt.ylim([-0.2, 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of oversmoothing\n",
    "Actually, the GP model only approximates trend of the function.\n",
    "All the curves are treated as noise.\n",
    "The knowledge about this (in fact there is some repeated structure) should be incorporated into the model via kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X = torch.rand(300, 2)\n",
    "y = torch.tensor(rastrigin(X.numpy()), dtype=torch.float32).squeeze()\n",
    "\n",
    "kernel = kernels.ScaleKernel(kernels.RBFKernel())\n",
    "model = GPRegressor(X, y, kernel)\n",
    "train(model, X, y)\n",
    "\n",
    "fig = plot_2d_func(lambda x: model.predict(torch.tensor(x).float()).mean.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Covariance functions\n",
    "\n",
    "To deal with this problem one should select a proper covariance function.\n",
    "\n",
    "Popular covariance functions are: `Exponential`, `Matern32`, `Matern52`, `RatQuad`, `Linear`, `Periodic`. \n",
    "They have different smoothness and can better suit for linear or periodic functions.\n",
    "\n",
    "* Exponential:\n",
    "$$\n",
    "k(x, x') = \\sigma^2 \\exp \\left (-\\frac{r}{l} \\right), \\quad r = \\|x - x'\\|\n",
    "$$\n",
    "\n",
    "* Matern32\n",
    "$$\n",
    "k(x, x') = \\sigma^2 \\left (1 + \\sqrt{3}\\frac{r}{l} \\right )\\exp \\left (-\\sqrt{3}\\frac{r}{l} \\right )\n",
    "$$\n",
    "\n",
    "* Matern52\n",
    "$$\n",
    "k(x, x') = \\sigma^2 \\left (1 + \\sqrt{5}\\frac{r}{l} + \\frac{5}{3}\\frac{r^2}{l^2} \\right ) \\exp \\left (-\\sqrt{5}\\frac{r}{l} \\right )\n",
    "$$\n",
    "\n",
    "* RatQuad\n",
    "$$\n",
    "k(x, x') = \\left ( 1 + \\frac{r^2}{2\\alpha l^2}\\right )^{-\\alpha}\n",
    "$$\n",
    "\n",
    "* Linear\n",
    "$$\n",
    "k(x, x') = \\sum_i \\sigma_i^2 x_i x_i'\n",
    "$$\n",
    "\n",
    "* Polynomial\n",
    "$$\n",
    "k(x, x') = \\sigma^2 (x^T x' + c)^d\n",
    "$$\n",
    "\n",
    "* Periodic\n",
    "$$\n",
    "k(x, x') = \\sigma^2 \\exp\\left ( -2 \\frac{\\sin^2(\\pi r)}{l^2}\\right ).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "covariance_functions = [kernels.RBFKernel(), kernels.SpectralMixtureKernel(2),\n",
    "                        kernels.MaternKernel(nu=5/2), kernels.LinearKernel(power=1),\n",
    "                        kernels.PolynomialKernel(power=2), kernels.PeriodicKernel(),\n",
    "                       ]\n",
    "figure, axes = plt.subplots(2, 3, figsize=(9, 6))\n",
    "axes = axes.ravel()\n",
    "for i, k in enumerate(covariance_functions):\n",
    "    plot_kernel(k, ax=axes[i])\n",
    "    axes[i].set_title(str(k).split('(')[0])\n",
    "figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combination of covariance functions\n",
    "\n",
    "* Sum of covariance function is a valid covariance function:\n",
    "\n",
    "$$\n",
    "k(x, x') = k_1(x, x') + k_2(x, x')\n",
    "$$\n",
    "\n",
    "* Product of covariance functions is a valid covariance funciton:\n",
    "$$\n",
    "k(x, x') = k_1(x, x') k_2(x, x')\n",
    "$$\n",
    "\n",
    "### Combinations of covariance functions in GPytorch\n",
    "\n",
    "In GPytorch to combine covariance functions you can just use operators `+` and `*`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot some of the combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_functions = [kernels.LinearKernel(power=1), kernels.PeriodicKernel(),\n",
    "                        kernels.RBFKernel()]\n",
    "operations = {'+': lambda x, y: x + y,\n",
    "              '*': lambda x, y: x * y}\n",
    "\n",
    "figure, axes = plt.subplots(len(operations), len(covariance_functions), figsize=(9, 6))\n",
    "\n",
    "import itertools\n",
    "axes = axes.ravel()\n",
    "count = 0\n",
    "for j, base_kernels in enumerate(itertools.combinations(covariance_functions, 2)):\n",
    "    for k, (op_name, op) in enumerate(operations.items()):\n",
    "        kernel = op(base_kernels[0], base_kernels[1])\n",
    "        plot_kernel(kernel, ax=axes[count])\n",
    "        kernel_names = [\n",
    "            str(base_kernels[i]).split('(')[0] for i in [0, 1]\n",
    "        ]\n",
    "        axes[count].set_title('{} {} {}'.format(kernel_names[0], op_name, kernel_names[1]),\n",
    "                              fontsize=14)\n",
    "        count += 1\n",
    "figure.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Additive kernels\n",
    "\n",
    "One of the popular approach to model the function of interest is to decompose the function into sum of univariate functions, bivariate functions and so on:\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^d f_i(x_i) + \\sum_{i < j} f_{ij}(x_i, x_j) + \\ldots\n",
    "$$\n",
    "\n",
    "The proposed structure of a function leads to more approriate kernel representation for it.\n",
    "Note, that this representation is often useful for functions with large number of input dimensions.\n",
    "\n",
    "**Example**: $\\quad f(x_1, x_2) = f_1(x_1) + f_2(x_2)$  \n",
    "To model it using GP use additive kernel $\\quad k(x, y) = k_1(x_1, y_1) + k_2(x_2, y_2)$.\n",
    "\n",
    "More general - add kernels each depending on subset of inputs\n",
    "$$\n",
    "k(x, y) = k_1(x, y) + \\ldots + k_D(x, y),\n",
    "$$\n",
    "where, for example, $k_1(x, x') = k_1(x_1, x_1'), \\; k_2(x, x') = k_2((x_1, x_3), (x_1', x_3'))$, etc.\n",
    "\n",
    "Here is an example of ${\\rm RBF}(x_1) + {\\rm RBF}(x_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create kernel\n",
    "k1 = kernels.RBFKernel(active_dims=[0])\n",
    "k2 = kernels.RBFKernel(active_dims=[1])\n",
    "\n",
    "kernel = k1 + k2\n",
    "\n",
    "# evaluate kernel on grid\n",
    "x = torch.meshgrid(torch.linspace(-3, 3, 50), torch.linspace(-3, 3, 50))\n",
    "x = torch.cat([x[0].reshape(-1, 1), x[1].reshape(-1, 1)], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = kernel(x, torch.tensor([[0., 0.]])).evaluate()\n",
    "\n",
    "# Plot kernel\n",
    "figure = plt.figure()\n",
    "ax = figure.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x[:, 0].reshape(50, 50).numpy(),\n",
    "                x[:, 1].reshape(50, 50).numpy(),\n",
    "                z.reshape(50, 50).numpy(), cmap=cm.jet)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kernels on arbitrary types of objects\n",
    "\n",
    "Kernels can be defined over all types of data structures: text, images, matrices, graphs, etc. You just need to define similarity between objects.\n",
    "Note: for GPR we need a positive definite kernel.\n",
    "\n",
    "#### Kernels on categorical data\n",
    "\n",
    "* Represent your categorical variable as a by a one-of-k encoding: $\\quad x = (x_1, \\ldots, x_k)$.\n",
    "* Use RBF kernel with `ARD=True`: $\\quad k(x , x') = \\sigma^2 \\prod_{i = 1}^k\\exp{\\left ( -\\dfrac{(x_i - x_i')^2}{\\sigma_i^2} \\right )}$. The lengthscale will now encode whether the rest of the function changes.\n",
    "* Short lengthscales for categorical variables mean, that your model is not sharing any information between data of different categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2 Sampling from GP\n",
    "\n",
    "So, you have defined some complex kernel.\n",
    "You can plot it to see how it looks and guess what kind of functions it can approximate.\n",
    "Another way to do it is to actually generate random functions using this kernel.\n",
    "\n",
    "GP defines distribution over functions, which is defined by its *mean function* $m(x)$ and *covariance function* $k(x, y)$: for any set $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\in \\mathbb{R}^d \\rightarrow$ $\\left (f(\\mathbf{x}_1), \\ldots, f(\\mathbf{x}_N) \\right ) \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{K})$,\n",
    "where $\\mathcal{m} = (m(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N)$, $\\mathbf{K} = \\|k(\\mathbf{x}_i, \\mathbf{x}_j)\\|_{i,j=1}^N$.\n",
    "\n",
    "Sampling procedure:\n",
    "\n",
    "1. Generate set of points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$.\n",
    "2. Calculate mean and covariance matrix $\\mathcal{m} = (m(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N)$, $\\mathbf{K} = \\|k(\\mathbf{x}_i, \\mathbf{x}_j)\\|_{i,j=1}^N$.\n",
    "3. Generate vector from multivariate normal distribution $\\mathcal{N}(\\mathbf{m}, \\mathbf{K})$.\n",
    "\n",
    "Below try to change RBF kernel to some other kernel and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "kernel = kernels.MaternKernel(nu=5 / 2)\n",
    "\n",
    "X = torch.linspace(0, 5, 500)\n",
    "\n",
    "mu = np.zeros(500)\n",
    "\n",
    "with torch.no_grad():\n",
    "    C = kernel(X).evaluate().numpy()\n",
    "\n",
    "Z = np.random.multivariate_normal(mu, C, 3)\n",
    "\n",
    "plt.figure()\n",
    "for i in range(3):\n",
    "    plt.plot(X.numpy(), Z[i, :])\n",
    "    plt.title(str(kernel).split('(')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task\n",
    "\n",
    "Build a GP model that predicts airline passenger counts on international flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = np.load('airline.npz')\n",
    "\n",
    "X = torch.tensor(data['X'])\n",
    "y = torch.tensor(data['y']).squeeze()\n",
    "\n",
    "train_indices = list(range(70)) + list(range(90, 129))\n",
    "test_indices = range(70, 90)\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(X_train.numpy(), y_train.numpy(), '.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to obtain something like this\n",
    "\n",
    "![image.png](airline_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's try RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see below it doesn't work ;("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = GPRegressor(X_train, y_train, k_rbf).double()\n",
    "train(model, X_train, y_train, n_epochs=500)\n",
    "\n",
    "for name, p in model.named_hyperparameters():\n",
    "    print(f\"{name}: {p.item():.3f}\")\n",
    "    \n",
    "xlim = [1948, 1964]\n",
    "utils.plot_model(model, xlim=xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will try to model this data set using 3 additive components: trend, seasonality and noise.  \n",
    "So, the kernel should be a sum of 3 kernels:  \n",
    "`kernel = kernel_trend + kernel_seasonality + kernel_noise`\n",
    "\n",
    "#### Let's first try to model trend\n",
    "\n",
    "Trend is almost linear with some small nonlinearity, so you can use sum of linear kernel with some other which gives this small nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = GPRegressor(X_train, y_train, k_trend).double()\n",
    "train(model, X_train, y_train, n_epochs=1000)\n",
    "\n",
    "for name, p in model.named_hyperparameters():\n",
    "    print(f\"{name}: {p.item():.3f}\")\n",
    "    \n",
    "utils.plot_model(model, xlim=xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's model periodicity\n",
    "Single periodic kernel fails (why?).\n",
    "Try to use product of periodic kernel with some other kernel (or maybe 2 other kernels).\n",
    "Note that the amplitude increases with x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kernel = kernels.AdditiveKernel(k_trend, k_seasonal)\n",
    "\n",
    "model = GPRegressor(X_train, y_train, kernel).double()\n",
    "\n",
    "train(model, X_train, y_train, n_epochs=1000)\n",
    "\n",
    "for name, p in model.named_hyperparameters():\n",
    "    print(f\"{name}: {p.item():.3f}\")\n",
    "    \n",
    "utils.plot_model(model, xlim=xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's add noise model\n",
    "The dataset is heteroscedastic, i.e. noise variance depends on x: it increases linearly with x.  \n",
    "To model homoscedastic white noise we implement `WhiteNoiseKernel`, but it assumes that noise variance is the same at every x.\n",
    "By what kernel it should be multiplied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.lazy import DiagLazyTensor, ZeroLazyTensor\n",
    "\n",
    "class WhiteNoiseKernel(kernels.Kernel):\n",
    "    def __init__(self, noise=1):\n",
    "        super().__init__()\n",
    "        self.noise = noise\n",
    "    \n",
    "    def forward(self, x1, x2, **params):\n",
    "        if self.training and torch.equal(x1, x2):\n",
    "            return DiagLazyTensor(torch.ones(x1.shape[0]).to(x1) * self.noise)\n",
    "        elif x1.size(-2) == x2.size(-2) and torch.equal(x1, x2):\n",
    "            return DiagLazyTensor(torch.ones(x1.shape[0]).to(x1) * self.noise)\n",
    "        else:\n",
    "            return torch.zeros(x1.shape[0], x2.shape[0]).to(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "kernel = kernels.AdditiveKernel(k_trend, k_seasonal, k_noise)\n",
    "\n",
    "model = GPRegressor(X_train, y_train, kernel).double()\n",
    "\n",
    "train(model, X_train, y_train, n_epochs=1000)\n",
    "\n",
    "for name, p in model.named_hyperparameters():\n",
    "    print(f\"{name}: {p.item():.3f}\")\n",
    "\n",
    "utils.plot_model(model, xlim=xlim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Bonus tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic covariance structure search\n",
    "We can construct kernel is automatic way.\n",
    "Here is our data set (almost the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test = np.where((X.numpy()[:,0] > 1957))[0]\n",
    "idx_train = np.where((X.numpy()[:,0] <= 1957))[0]\n",
    "X_train = X[idx_train]\n",
    "y_train = y[idx_train]\n",
    "\n",
    "X_test = X[idx_test]\n",
    "y_test = y[idx_test]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(X_train.numpy(), y_train.numpy(), '.', color='red');\n",
    "plt.plot(X_test.numpy(), y_test.numpy(), '.', color='green');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.numpy().reshape(-1, 1))\n",
    "y_train = torch.tensor(y_train).squeeze()\n",
    "y_test = torch.tensor(scaler_y.transform(y_test.numpy().reshape(-1, 1))).squeeze()\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "X_train = scaler_x.fit_transform(X_train.numpy().reshape(-1, 1))\n",
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(scaler_x.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_learned(model, X_test, y_test):\n",
    "    utils.plot_model(model, xlim=[-2, 3])\n",
    "    plt.plot(X_test.numpy(), y_test.numpy(), '.', color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressing Sturcture Through Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "$$\n",
    "\\underbrace{\\text{RBF}\\times\\text{Lin}}_\\text{increasing trend} + \\underbrace{\\text{RBF}\\times\\text{Per}}_\\text{varying-amplitude periodic} + \\underbrace{\\text{RBF}}_\\text{residual}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy Searching for the Optimum Kernel Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can wonder: how to automatically search the kernel structure? We can optimize some criteria, which balance between a loss function value and the complexity of the model.\n",
    "Reasinobale candidate for this is BIC-criteria:\n",
    "\n",
    "$$\n",
    "BIC = - 2. \\text{Log-Liklihood} + m \\cdot\\log{n}\n",
    "$$\n",
    "\n",
    "where $n$ sample size and $m$ number of the parameters.\n",
    "\n",
    "However, the procedure of fitting Gaussian Process is quite expensive $O(n^3)$. Hence,  instead of the combinatorial search through all possible combinations, we grow the kernel structure greedy.\n",
    "\n",
    "You can find more details at the https://github.com/jamesrobertlloyd/gp-structure-search. For now, we present toy-example algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the set of operations:\n",
    "\n",
    "$$\n",
    "\\text{Algebra: } +,\\times\n",
    "$$\n",
    "\n",
    "and the set of basic kernels:\n",
    "\n",
    "$$\n",
    "\\text{Kernels: } \\text{Poly}, \\text{RBF}, \\text{Periodic}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each level we select extenstion of our current kernel with the lowest BIC. This is an example of the possible kernel grow process (mark notes the lowest BIC at the level):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='gp.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task*\n",
    "Implement function that trains a model with given kernel and dataset, calculates and returns BIC\n",
    "The marginal log-lilkelihood of the model can be calculated using `gpytorch.mlls.ExactMarginalLogLikelihood`,\n",
    "number of parameters of the model you can get by counting number of `model.hyperparameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def train_model_get_bic(X_train, y_train, kernel, n_epochs=300):\n",
    "    \"\"\"\n",
    "    Train GP model and calculate Bayesian Information Criterion (BIC)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : torch.tensor\n",
    "        Array of train features, n*d (d>=1)\n",
    "    \n",
    "    y_train : torch.tensor\n",
    "        Array of target values\n",
    "        \n",
    "    kernel : gpytorch.kernels.Kernel\n",
    "        Kernel object\n",
    "        \n",
    "    n_epochs : int\n",
    "        Number of epochs to train GP model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bic : float\n",
    "        BIC value\n",
    "    \"\"\"\n",
    "    kernel = copy.deepcopy(kernel)\n",
    "    \n",
    "    ######## Your code here ########\n",
    "    \n",
    "    bic = ...\n",
    "    \n",
    "    ### Your code ends here ###\n",
    "\n",
    "    return bic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a utility function which take list of kernels and operations between them, calculates all product kernels\n",
    "and returns a list of them.\n",
    "After that we need only take sum of the kernels from this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_all_product_kernels(op_list, kernel_list):\n",
    "    \"\"\"\n",
    "    Find product pairs and calculate them.\n",
    "    For example, if we are given expression:\n",
    "        K = k1 * k2 + k3 * k4 * k5\n",
    "    the function will calculate all the product kernels\n",
    "        k_mul_1 = k1 * k2\n",
    "        k_mul_2 = k3 * k4 * k5\n",
    "    and return list [k_mul_1, k_mul_2].\n",
    "    \"\"\"\n",
    "    product_index = np.where(np.array(op_list) == '*')[0]\n",
    "    if len(product_index) == 0:\n",
    "        return kernel_list\n",
    "\n",
    "    product_index = product_index[0]\n",
    "    product_kernel = kernel_list[product_index] * kernel_list[product_index + 1]\n",
    "    \n",
    "    if len(op_list) == product_index + 1:\n",
    "        kernel_list_copy = kernel_list[:product_index] + [product_kernel]\n",
    "        op_list_copy = op_list[:product_index]\n",
    "    else:\n",
    "        kernel_list_copy = kernel_list[:product_index] + [product_kernel] + kernel_list[product_index + 2:]\n",
    "        op_list_copy = op_list[:product_index] + op_list[product_index + 1:]\n",
    "        \n",
    "    return _get_all_product_kernels(op_list_copy, kernel_list_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task*\n",
    "\n",
    "This is the main class, you need to implement several methods inside\n",
    "1. method `init_kernel()` - this function constructs initial model, i.e. the model with one kernel. You need just iterate through the list of base kernels and choose the best one according to BIC\n",
    "2. method `grow_level()` - this function adds new level. You need to iterate through all base kernels and all operations,\n",
    "apply each operation to the previously constructed kernel and each base kernel (use method `_make_kernel()` for this) and then choose the best one according to BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyKernel:\n",
    "    \"\"\"\n",
    "    Class for greedy growing kernel structure\n",
    "    \"\"\"\n",
    "    def __init__(self, algebra, base_kernels):\n",
    "        self.algebra = algebra\n",
    "        self.base_kernels = base_kernels\n",
    "        self.kernel = None\n",
    "        self.kernel_list = []\n",
    "        self.op_list = []\n",
    "        self.str_kernel = None\n",
    "    \n",
    "    def _make_kernel(self, op_list, kernel_list):\n",
    "        \"\"\"\n",
    "        Create kernel according to operation and kernel lists\n",
    "        \"\"\"\n",
    "        kernels_to_sum = _get_all_product_kernels(op_list, kernel_list)\n",
    "        new_kernel = kernels_to_sum[0]\n",
    "        for k in kernels_to_sum[1:]:\n",
    "            new_kernel = new_kernel + k\n",
    "        return new_kernel\n",
    "    \n",
    "    def init_kernel(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Find initial single best kernel\n",
    "        \"\"\"\n",
    "        best_kernel = None\n",
    "        \n",
    "        ###### Your code here ######\n",
    "\n",
    "        # You need just iterate through the list of base kernels and choose the best one according to BIC\n",
    "        # save the kernel in `best_kernel` variable\n",
    "        \n",
    "        # base kernels are given by self.base_kernels --- list of kernel objects\n",
    "        \n",
    "        \n",
    "        ### Your code ends here ###\n",
    "        \n",
    "        assert best_kernel is not None\n",
    "        \n",
    "        self.kernel_list.append(best_kernel)\n",
    "        self.str_kernel = str(best_kernel.base_kernel).split('(')[0]\n",
    "        \n",
    "    def grow_level(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Select optimal extension of current kernel (add one new kernel)\n",
    "        \"\"\"\n",
    "        \n",
    "        best_kernel = None  # should be kernel object\n",
    "        best_op = None  # should be operation name, i.e. \"+\" or \"*\"\n",
    "        \n",
    "        ###### Your code here ######\n",
    "        \n",
    "        # You need to iterate through all base kernels and all operations,\n",
    "        # apply each operation to the previously constructed kernel and each base kernel\n",
    "        # (use method `_make_kernel()` for this) and then choose the best one according to BIC.\n",
    "        \n",
    "        # base kernels are given by self.base_kernels --- list of kernel objects\n",
    "        # operations are given by self.algebra --- dictionary:\n",
    "        #                                              {\"+\": lambda x, y: x + y\n",
    "        #                                               \"*\": lambda x, y: x * y}\n",
    "\n",
    "        # best_kernel - kernel object, store in this variable the best found kernel\n",
    "        # best_op - '+' or '*', store in this variable the best found operation\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        ### Your code ends here ###\n",
    "\n",
    "        assert best_kernel is not None\n",
    "        assert best_op is not None\n",
    "        \n",
    "        self.kernel_list.append(best_kernel)\n",
    "        self.op_list.append(best_op)\n",
    "        \n",
    "        new_kernel = self._make_kernel(self.op_list, self.kernel_list)\n",
    "        str_new_kernel = '{} {} {}'.format(self.str_kernel, best_op,\n",
    "                                           str(best_kernel.base_kernel).split('(')[0])\n",
    "        \n",
    "        return new_kernel, str_new_kernel\n",
    "    \n",
    "    def grow_tree(self, X_train, y_train, max_depth):\n",
    "        \"\"\"\n",
    "        Greedy kernel construction\n",
    "        \"\"\"\n",
    "        if self.kernel == None:\n",
    "            self.init_kernel(X_train, y_train)\n",
    "            \n",
    "        for i in range(max_depth):\n",
    "            self.kernel, self.str_kernel = self.grow_level(X_train, y_train)\n",
    "            print(self.str_kernel)\n",
    "            \n",
    "    def fit_model(self, X_train, y_train, kernel, n_epochs):\n",
    "        model = GPRegressor(X_train, y_train, kernel).double()\n",
    "        train(model, X_train, y_train, n_epochs=n_epochs)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define the algebra and list of base kernels.\n",
    "To make learning process more robust we constrain some parameters of the kernels to lie within\n",
    "some reasonable intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operations under kernels:\n",
    "from gpytorch.constraints import Interval\n",
    "\n",
    "algebra = {'+': lambda x, y: x + y,\n",
    "           '*': lambda x, y: x * y\n",
    "          }\n",
    "\n",
    "# basic kernels list:\n",
    "poly_kern = kernels.ScaleKernel(kernels.PolynomialKernel(power=1))\n",
    "periodic_kern = kernels.ScaleKernel(kernels.PeriodicKernel())\n",
    "rbf_kern = kernels.ScaleKernel(kernels.RBFKernel(input_dim=1))\n",
    "\n",
    "kernels_list = [poly_kern, periodic_kern, rbf_kern]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model.\n",
    "You should obtain something which is more accurate than the trend model ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GK = GreedyKernel(algebra, kernels_list)\n",
    "GK.grow_tree(X_train, y_train, 4)\n",
    "model = GK.fit_model(X_train, y_train, GK.kernel, n_epochs=500)\n",
    "plot_model_learned(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task\n",
    "Try to approximate rastrigin function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plot_2d_func(rastrigin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "X = torch.rand(200, 2, dtype=torch.float64)\n",
    "y = torch.tensor(rastrigin(X.numpy()), dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: you can constrain parameters of the covariance functions, for example\n",
    "`model.std_periodic.period.constrain_bounded(0, 0.2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Your code here ########\n",
    "\n",
    "\n",
    "### Your code ends here ###\n",
    "\n",
    "with gpytorch.settings.fast_computations(False, False, False):\n",
    "    train(model, X, y, n_epochs=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p in model.named_hyperparameters():\n",
    "    print(name, f\"{p.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.rand(1000, 2).double()\n",
    "y_test = rastrigin(x_test.numpy())\n",
    "\n",
    "y_pr = model.predict(x_test).mean.numpy()\n",
    "\n",
    "mse = mean_squared_error(y_test.ravel(), y_pr.ravel())\n",
    "print('MSE: {}'.format(mse))\n",
    "\n",
    "fig = plot_2d_func(lambda x: model.predict(torch.tensor(x)).mean.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appendix: Gaussian Process Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification\n",
    "\n",
    "A data set $\\left (X, \\mathbf{y} \\right ) = \\left \\{ (x_i, y_i), x_i \\in \\mathbb{R}^d, y_i \\in \\{+1, -1\\} \\right \\}_{i = 1}^N$ is given.  \n",
    "\n",
    "Assumption:\n",
    "$$\n",
    "p(y = +1 \\; | \\; x) = \\sigma(f(x)) = \\pi(x),\n",
    "$$\n",
    "where latent function $f(x)$ is a Gaussian Processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to produce a probabilistic prediction\n",
    "$$\n",
    "\\pi_* = p(y_* \\; | \\; X, \\mathbf{y}, x_*) = \\int \\sigma(f_*) p(f_* \\; | \\; X, \\mathbf{y}, x_*) df_*,\n",
    "$$\n",
    "$$\n",
    "p(f_* \\; | \\; X, \\mathbf{y}, x_*) = \\int p(f_* \\; | \\; X, x_*, \\mathbf{f}) p(\\mathbf{f} \\; | \\; X, \\mathbf{y}) d\\mathbf{f},\n",
    "$$\n",
    "where $p(\\mathbf{f} \\; |\\; X, \\mathbf{y}) = \\dfrac{p(\\mathbf{y} | X, \\mathbf{f}) p(\\mathbf{f} | X)}{p(\\mathbf{y} | X)}$ is the posterior over the latent variables.\n",
    "\n",
    "Both integrals are intractable.\n",
    "\n",
    "Use approximation technique like Laplace approximation, Expectation Propagation or Variational Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "def cylinder(x):\n",
    "    y = (1 / 7.0 - (x[:, 0] - 0.5)**2 - (x[:, 1] - 0.5)**2) > 0\n",
    "    return y\n",
    "\n",
    "np.random.seed(42)\n",
    "X = torch.rand(40, 2, dtype=torch.float64)\n",
    "y = torch.from_numpy(cylinder(X.numpy())).double()\n",
    "\n",
    "x_grid = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n",
    "y_grid = cylinder(np.hstack((x_grid[0].reshape(-1, 1), x_grid[1].reshape(-1, 1)))).reshape(x_grid[0].shape)\n",
    "\n",
    "positive_idx = y.numpy() == 1\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(X.numpy()[positive_idx, 0], X.numpy()[positive_idx, 1], '.', markersize=10, label='Positive')\n",
    "plt.plot(X.numpy()[~positive_idx, 0], X.numpy()[~positive_idx, 1], '.', markersize=10, label='Negative')\n",
    "im = plt.contour(x_grid[0], x_grid[1], y_grid, 10, cmap=cm.hot)\n",
    "plt.colorbar(im)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "class GPClassifier(AbstractVariationalGP):\n",
    "    def __init__(self, X, y, kernel, likelihood=None):\n",
    "        variational_distribution = CholeskyVariationalDistribution(X.shape[0])\n",
    "        variational_strategy = VariationalStrategy(self, X, variational_distribution)\n",
    "        if likelihood is None:\n",
    "            likelihood = gpytorch.likelihoods.BernoulliLikelihood()\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean = gpytorch.means.ConstantMean()\n",
    "        self.kernel = kernel\n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean(x)\n",
    "        covar_x = self.kernel(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = self(X)\n",
    "            return self.likelihood(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "def train_classifier(model, X, y, n_epochs=100, fix_noise_variance=None, verbose=True):\n",
    "    model.train()\n",
    "        \n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=0.1)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.VariationalELBO(model.likelihood, model, y.shape[0])\n",
    "\n",
    "    with tqdm.trange(n_epochs, disable=not verbose) as bar:\n",
    "        for i in bar:\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(X)\n",
    "            loss = -mll(out, y)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            bar.set_postfix(Loss=f\"{loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_2d(model):\n",
    "\n",
    "    size = 100\n",
    "    x_grid = torch.meshgrid(torch.linspace(0, 1, size),\n",
    "                            torch.linspace(0, 1, size))\n",
    "    x_grid = torch.cat((x_grid[0].reshape(-1, 1),\n",
    "                        x_grid[1].reshape(-1, 1)), dim=1)\n",
    "    y_grid = cylinder(x_grid)\n",
    "    y_grid = y_grid.reshape(size, -1).numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model.likelihood(model(x_grid.double()))\n",
    "        mean = prediction.mean.numpy().reshape(size, -1)\n",
    "        mean = mean > 0.5\n",
    "    \n",
    "    x_grid = x_grid.numpy()\n",
    "    \n",
    "    plt.contourf(x_grid[:, 0].reshape(size, -1),\n",
    "                 x_grid[:, 1]. reshape(size, -1),\n",
    "                 mean, levels=40)\n",
    "    \n",
    "    plt.plot(X.numpy()[positive_idx, 0], X.numpy()[positive_idx, 1], '.', markersize=10, label='Positive')\n",
    "    plt.plot(X.numpy()[~positive_idx, 0], X.numpy()[~positive_idx, 1], '.', markersize=10, label='Negative')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "kernel = kernels.ScaleKernel(kernels.RBFKernel(ard_num_dims=2))\n",
    "\n",
    "model = GPClassifier(X, y, kernel).double()\n",
    "train_classifier(model, X, y, n_epochs=100)\n",
    "    \n",
    "plot_model_2d(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's change lengthscale to some small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.kernel.base_kernel.lengthscale = torch.tensor([[0.05, 0.05]]).double()\n",
    "plot_model_2d(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
